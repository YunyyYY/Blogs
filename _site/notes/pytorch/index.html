
<!doctype>
<html lang="en">
  <head>
    <meta content='Pytorch - Yunyy' name='title' />
    <meta content='Pytorch - Yunyy' name='og:title' />
    <title>Pytorch - Yunyy</title>
    <link href='http://localhost:4000/images/fav.png' rel='shortcut icon'>
<link href='http://localhost:4000/stylesheets/style.css' rel='stylesheet' type='text/css' />
<link href='http://localhost:4000/stylesheets/syntax.css' rel='stylesheet' type='text/css' />

<meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type' />

  <meta content='http://localhost:4000/notes/pytorch/' property='og:url' />
  <meta content="PyTorch problems collect  cited from 60 min blitz1. NumPy BridgeConverting a Torch Tensor to a NumPy array and vice v..." property='og:description' />
  <meta content="article" property="og:type" />

<!-- - -->





  </head>
  <body class="lh-copy dark-gray pa0 f6 sans-serif bg-super-white">
    <header class="tc mt4">
      <a href="http://localhost:4000">
        <img src="http://localhost:4000/images/fav.png" alt="Home" width="75" height="75">
      </a>
      <h1>Pytorch</h1>
    </header>
    <div class="mw7 bg-white mt4 mb3 center br2-ns bt bb ba-ns b--light-gray">
      <nav class="bb b--light-gray pv4 tc" aria-label="Main">
        
          <a class="link blue hover-mid-gray mh2 pv1"
             href="http://localhost:4000/">
             Home
           </a>
        
          <a class="link blue hover-mid-gray mh2 pv1"
             href="http://localhost:4000/categories">
             Blogs
           </a>
        
          <a class="link blue hover-mid-gray mh2 pv1"
             href="http://localhost:4000/about">
             About
           </a>
        
      </nav>

      <main class="tl f6 relative pa4 pa5-ns overflow-hidden">
        
        <div class="markdown-body">
          <article itemscope itemtype="">
    <div itemprop="articleBody">
        <h2 id="pytorch-problems-collect">PyTorch problems collect</h2>

<blockquote>
  <p>cited from <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">60 min blitz</a></p>
</blockquote>

<h3 id="1-numpy-bridge">1. NumPy Bridge</h3>

<p>Converting a Torch Tensor to a NumPy array and vice versa:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div>

<p>The Torch Tensor and NumPy array will <strong>share their underlying memory locations</strong> (if the Torch Tensor is on CPU), and changing one will change the other.</p>

<h3 id="2-cuda-tensors">2. CUDA Tensors</h3>

<p>Tensors can be moved onto any device using the <code class="highlighter-rouge">.to</code> method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># let us run this cell only if CUDA is available
# We will use ``torch.device`` objects to move tensors in and out of GPU
</span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span>          <span class="c1"># a CUDA device object
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># directly create a tensor on GPU
</span>    <span class="c1"># could simply use x.cuda()
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>                       <span class="c1"># or just use strings ``.to("cuda")``
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">))</span>       <span class="c1"># ``.to`` can also change dtype together!
</span></code></pre></div></div>

<p>Out:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0.6313], device='cuda:0')
tensor([0.6313], dtype=torch.float64)
</code></pre></div></div>

<h3 id="3-autograd-package">3. <code class="highlighter-rouge">autograd</code> package</h3>

<p><code class="highlighter-rouge">torch.Tensor</code> is the central class of the package. If you set its attribute <code class="highlighter-rouge">.requires_grad</code> as <code class="highlighter-rouge">True</code>, it starts to track all operations on it. The tensors created as a result of an operation wiil have a <code class="highlighter-rouge">grad_fn</code>.When you finish your computation you can call <code class="highlighter-rouge">.backward()</code> and have all the gradients computed automatically. The gradient for this tensor will be accumulated into <code class="highlighter-rouge">.grad</code> attribute.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="gradients">Gradients</h4>

<p>Call <code class="highlighter-rouge">.backward()</code> to backprop.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<p>Because <code class="highlighter-rouge">out</code> contains a single scalar, <code class="highlighter-rouge">out.backward()</code> is equivalent to <code class="highlighter-rouge">out.backward(torch.tensor(1.))</code>.</p>

<p>Print gradients d(out)/dx:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<p>Mathematically, if you have a vector valued function $\vec{y}=f(\vec{x})$, then the gradient of $\vec{y}$ with respect to $\vec{x}$ is a Jacobian matrix:
<script type="math/tex">% <![CDATA[
J=\left(\begin{array}{ccc}{\frac{\partial y_{1}}{\partial x_{1}}} & {\cdots} & {\frac{\partial y_{1}}{\partial x_{n}}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial y_{m}}{\partial x_{1}}} & {\cdots} & {\frac{\partial y_{m}}{\partial x_{n}}}\end{array}\right) %]]></script>
Generally speaking, <code class="highlighter-rouge">torch.autograd</code> is an engine for computing vector-Jacobian product. Given a vector $v=\left(\begin{array}{llll}{v_{1}} &amp; {v_{2}} &amp; {\cdots} &amp; {v_{m}}\end{array}\right)^{T}$, it computes $v^{T} \cdot J$. If $\vec{v}$ happens to be the gradient of a scalar function $l=g(\vec{y})$, that is, $v=\left(\begin{array}{ccc}{\frac{\partial l}{\partial y_{1}}} &amp; {\dots} &amp; {\frac{\partial l}{\partial y_{m}}}\end{array}\right)^{T}$, then by the chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\vec{x}$:
<script type="math/tex">% <![CDATA[
J^{T} \cdot v=\left(\begin{array}{ccc}{\frac{\partial y_{1}}{\partial x_{1}}} & {\cdots} & {\frac{\partial y_{m}}{\partial x_{1}}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial y_{1}}{\partial x_{n}}} & {\cdots} & {\frac{\partial y_{m}}{\partial x_{n}}}\end{array}\right)\left(\begin{array}{c}{\frac{\partial l}{\partial y_{1}}} \\ {\vdots} \\ {\frac{\partial l}{\partial y_{m}}}\end{array}\right)=\left(\begin{array}{c}{\frac{\partial l}{\partial x_{1}}} \\ {\vdots} \\ {\frac{\partial l}{\partial x_{n}}}\end{array}\right) %]]></script>
For example,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="k">while</span> <span class="n">y</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">1000</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div>

<p>Now in this case <code class="highlighter-rouge">y</code> is no longer a scalar. <code class="highlighter-rouge">torch.autograd</code> could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to <code class="highlighter-rouge">backward</code> as argument:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="4-neural-networks">4. Neural networks</h3>

<p>Neural networks can be constructed using the <code class="highlighter-rouge">torch.nn</code> package.</p>

<p><code class="highlighter-rouge">nn</code> depends on <code class="highlighter-rouge">autograd</code> to define models and differentiate them. An <code class="highlighter-rouge">nn.Module</code> contains layers, and a method <code class="highlighter-rouge">forward(input)</code>that returns the <code class="highlighter-rouge">output</code>.</p>

<p>A typical training procedure for a neural network is as follows:</p>

<ul>
  <li>Define the neural network that has some learnable parameters (or weights)</li>
  <li>Iterate over a dataset of inputs</li>
  <li>Process input through the network</li>
  <li>Compute the loss (how far is the output from being correct)</li>
  <li>Propagate gradients back into the network’s parameters</li>
  <li>Update the weights of the network, typically using a simple update rule: <code class="highlighter-rouge">weight = weight - learning_rate * gradient</code></li>
</ul>

<h4 id="torchnnconv2d"><code class="highlighter-rouge">torch.nn.Conv2d</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>Collected problems</p>
</blockquote>

<h3 id="5-nnvariable">5. <code class="highlighter-rouge">nn.Variable</code></h3>

<h4 id="nnparameter"><code class="highlighter-rouge">nn.Parameter</code></h4>

<p>Parameter, in its raw form, is a tensor i.e. a multi dimensional matrix. It sub-classes the Variable class.</p>

<h3 id="6-nnforward">6. <code class="highlighter-rouge">nn.forward</code></h3>

<p>In an outer forward function,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nodes</span><span class="p">):</span>
	<span class="n">embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>  <span class="c1"># call self.enc.forward(nodes)
</span>	<span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">embeds</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">scores</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
</code></pre></div></div>

<p>When you call the model directly, the internal <code class="highlighter-rouge">__call__</code> function is used. This function manages all registered hooks and calls forward afterwards. That’s also the reason you should call the model directly, because otherwise your hooks might not work etc.</p>

<h3 id="7-multi-label-loss-function">7. Multi-label loss function</h3>


    </div>
</article>
        </div>
      </main>
    </div>
    <footer class="mw7 center tc pt3 pb1 silver">
        <a href="http://github.com/muan/scribble" class="link silver hover-blue pv1">笔记</a>
      <br><img src="http://localhost:4000/images/fav2.png" alt="Yunyy" class="pt1 pb1 center" />
    </footer>
  </body>
</html>
